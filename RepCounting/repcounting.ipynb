{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romil.punetha/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Javascript\n",
    "from IPython.display import SVG\n",
    "\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "from google.colab import drive\n",
    "from google.colab import output\n",
    "from google.colab.output import eval_js\n",
    "\n",
    "'''OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/'''\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils\n",
    "\n",
    "# get model\n",
    "\n",
    "def wget(url, path):\n",
    "    \"\"\"\n",
    "    Source from https://stackoverflow.com/questions/37573483/progress-bar-while-download-file-over-http-with-requests\n",
    "    Args:\n",
    "        url (str):\n",
    "        path (str):\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True, desc=f\"Downloading {url} to {path} ...\")\n",
    "    with open(path, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "\n",
    "def get_model(weight_root):\n",
    "    os.makedirs(weight_root, exist_ok=True)\n",
    "\n",
    "    weight_urls = [\n",
    "        \"https://storage.googleapis.com/repnet_ckpt/checkpoint\",\n",
    "        \"https://storage.googleapis.com/repnet_ckpt/ckpt-88.data-00000-of-00002\",\n",
    "        \"https://storage.googleapis.com/repnet_ckpt/ckpt-88.data-00001-of-00002\",\n",
    "        \"https://storage.googleapis.com/repnet_ckpt/ckpt-88.index\"\n",
    "    ]\n",
    "    for url in weight_urls:\n",
    "        path = f\"{weight_root}/{url.split('/')[-1]}\"\n",
    "        if os.path.isfile(path):\n",
    "            continue\n",
    "\n",
    "        wget(url, path)\n",
    "\n",
    "    return get_repnet_model(weight_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformer\n",
    "\n",
    "# Model definition\n",
    "layers = tf.keras.layers\n",
    "regularizers = tf.keras.regularizers\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "# Transformer from https://www.tensorflow.org/tutorials/text/transformer .\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    outputs: shape == (..., seq_len_q, depth_v)\n",
    "    attention_weights: shape == (..., seq_len_q, seq_len_k)\n",
    "  \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk.\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    # (..., seq_len_q, seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    outputs = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return outputs, attention_weights\n",
    "\n",
    "\n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Implements a single transformer layer (https://arxiv.org/abs/1706.03762).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dff,\n",
    "                 dropout_rate=0.1,\n",
    "                 reorder_ln=False):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.reorder_ln = reorder_ln\n",
    "\n",
    "    def call(self, x):\n",
    "        inp_x = x\n",
    "\n",
    "        if self.reorder_ln:\n",
    "            x = self.layernorm1(x)\n",
    "\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        attn_output, _ = self.mha(x, x, x, mask=None)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "\n",
    "        if self.reorder_ln:\n",
    "            out1 = inp_x + attn_output\n",
    "            x = out1\n",
    "        else:\n",
    "            # (batch_size, input_seq_len, d_model)\n",
    "            out1 = self.layernorm1(x + attn_output)\n",
    "            x = out1\n",
    "\n",
    "        if self.reorder_ln:\n",
    "            x = self.layernorm2(x)\n",
    "\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        if self.reorder_ln:\n",
    "            out2 = out1 + ffn_output\n",
    "        else:\n",
    "            # (batch_size, input_seq_len, d_model)\n",
    "            out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Multi-headed attention layer.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(\n",
    "            q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(\n",
    "            k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(\n",
    "            v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(\n",
    "            scaled_attention,\n",
    "            perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention,\n",
    "            (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        outputs = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return outputs, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resnet Period Estimator\n",
    "layers = tf.keras.layers\n",
    "regularizers = tf.keras.regularizers\n",
    "\n",
    "\n",
    "def flatten_sequential_feats(x, batch_size, seq_len):\n",
    "    \"\"\"Flattens sequential features with known batch size and seq_len.\"\"\"\n",
    "    x = tf.reshape(x, [batch_size, seq_len, -1])\n",
    "    return x\n",
    "\n",
    "\n",
    "def pairwise_l2_distance(a, b):\n",
    "    \"\"\"Computes pairwise distances between all rows of a and all rows of b.\"\"\"\n",
    "    norm_a = tf.reduce_sum(tf.square(a), 1)\n",
    "    norm_a = tf.reshape(norm_a, [-1, 1])\n",
    "    norm_b = tf.reduce_sum(tf.square(b), 1)\n",
    "    norm_b = tf.reshape(norm_b, [1, -1])\n",
    "    dist = tf.maximum(norm_a - 2.0 * tf.matmul(a, b, False, True) + norm_b, 0.0)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def get_sims(embs, temperature):\n",
    "    \"\"\"Calculates self-similarity between batch of sequence of embeddings.\"\"\"\n",
    "    batch_size = tf.shape(embs)[0]\n",
    "    seq_len = tf.shape(embs)[1]\n",
    "    embs = tf.reshape(embs, [batch_size, seq_len, -1])\n",
    "\n",
    "    def _get_sims(embs):\n",
    "        \"\"\"Calculates self-similarity between sequence of embeddings.\"\"\"\n",
    "        dist = pairwise_l2_distance(embs, embs)\n",
    "        sims = -1.0 * dist\n",
    "        return sims\n",
    "\n",
    "    sims = tf.map_fn(_get_sims, embs)\n",
    "    sims /= temperature\n",
    "    sims = tf.nn.softmax(sims, axis=-1)\n",
    "    sims = tf.expand_dims(sims, -1)\n",
    "    return sims\n",
    "\n",
    "\n",
    "class ResnetPeriodEstimator(tf.keras.models.Model):\n",
    "    \"\"\"RepNet model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_frames=64,\n",
    "            image_size=112,\n",
    "            base_model_layer_name='conv4_block3_out',\n",
    "            temperature=13.544,\n",
    "            dropout_rate=0.25,\n",
    "            l2_reg_weight=1e-6,\n",
    "            temporal_conv_channels=512,\n",
    "            temporal_conv_kernel_size=3,\n",
    "            temporal_conv_dilation_rate=3,\n",
    "            conv_channels=32,\n",
    "            conv_kernel_size=3,\n",
    "            transformer_layers_config=((512, 4, 512),),\n",
    "            transformer_dropout_rate=0.0,\n",
    "            transformer_reorder_ln=True,\n",
    "            period_fc_channels=(512, 512),\n",
    "            within_period_fc_channels=(512, 512)):\n",
    "        super(ResnetPeriodEstimator, self).__init__()\n",
    "\n",
    "        # Model params.\n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.base_model_layer_name = base_model_layer_name\n",
    "\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg_weight = l2_reg_weight\n",
    "\n",
    "        self.temporal_conv_channels = temporal_conv_channels\n",
    "        self.temporal_conv_kernel_size = temporal_conv_kernel_size\n",
    "        self.temporal_conv_dilation_rate = temporal_conv_dilation_rate\n",
    "\n",
    "        self.conv_channels = conv_channels\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        # Transformer config in form of (channels, heads, bottleneck channels).\n",
    "        self.transformer_layers_config = transformer_layers_config\n",
    "        self.transformer_dropout_rate = transformer_dropout_rate\n",
    "        self.transformer_reorder_ln = transformer_reorder_ln\n",
    "\n",
    "        self.period_fc_channels = period_fc_channels\n",
    "        self.within_period_fc_channels = within_period_fc_channels\n",
    "\n",
    "        # Base ResNet50 Model.\n",
    "        base_model = tf.keras.applications.ResNet50V2(\n",
    "            include_top=False, weights=None, pooling='max')\n",
    "        self.base_model = tf.keras.models.Model(\n",
    "            inputs=base_model.input,\n",
    "            outputs=base_model.get_layer(self.base_model_layer_name).output)\n",
    "\n",
    "        # 3D Conv on k Frames\n",
    "        self.temporal_conv_layers = [\n",
    "            layers.Conv3D(self.temporal_conv_channels,\n",
    "                          self.temporal_conv_kernel_size,\n",
    "                          padding='same',\n",
    "                          dilation_rate=(self.temporal_conv_dilation_rate, 1, 1),\n",
    "                          kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "                          kernel_initializer='he_normal')]\n",
    "        self.temporal_bn_layers = [layers.BatchNormalization()\n",
    "                                   for _ in self.temporal_conv_layers]\n",
    "\n",
    "        # Counting Module (Self-sim > Conv > Transformer > Classifier)\n",
    "        self.conv_3x3_layer = layers.Conv2D(self.conv_channels,\n",
    "                                            self.conv_kernel_size,\n",
    "                                            padding='same',\n",
    "                                            activation=tf.nn.relu)\n",
    "\n",
    "        channels = self.transformer_layers_config[0][0]\n",
    "        self.input_projection = layers.Dense(\n",
    "            channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "            activation=None)\n",
    "        self.input_projection2 = layers.Dense(\n",
    "            channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "            activation=None)\n",
    "\n",
    "        length = self.num_frames\n",
    "        self.pos_encoding = tf.compat.v1.get_variable(\n",
    "            name='resnet_period_estimator/pos_encoding',\n",
    "            shape=[1, length, 1],\n",
    "            initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
    "        self.pos_encoding2 = tf.compat.v1.get_variable(\n",
    "            name='resnet_period_estimator/pos_encoding2',\n",
    "            shape=[1, length, 1],\n",
    "            initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "        self.transformer_layers = []\n",
    "        for d_model, num_heads, dff in self.transformer_layers_config:\n",
    "            self.transformer_layers.append(\n",
    "                TransformerLayer(d_model, num_heads, dff,\n",
    "                                 self.transformer_dropout_rate,\n",
    "                                 self.transformer_reorder_ln))\n",
    "\n",
    "        self.transformer_layers2 = []\n",
    "        for d_model, num_heads, dff in self.transformer_layers_config:\n",
    "            self.transformer_layers2.append(\n",
    "                TransformerLayer(d_model, num_heads, dff,\n",
    "                                 self.transformer_dropout_rate,\n",
    "                                 self.transformer_reorder_ln))\n",
    "\n",
    "        # Period Prediction Module.\n",
    "        self.dropout_layer = layers.Dropout(self.dropout_rate)\n",
    "        num_preds = self.num_frames // 2\n",
    "        self.fc_layers = []\n",
    "        for channels in self.period_fc_channels:\n",
    "            self.fc_layers.append(layers.Dense(\n",
    "                channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "                activation=tf.nn.relu))\n",
    "        self.fc_layers.append(layers.Dense(\n",
    "            num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\n",
    "\n",
    "        # Within Period Module\n",
    "        num_preds = 1\n",
    "        self.within_period_fc_layers = []\n",
    "        for channels in self.within_period_fc_channels:\n",
    "            self.within_period_fc_layers.append(layers.Dense(\n",
    "                channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "                activation=tf.nn.relu))\n",
    "        self.within_period_fc_layers.append(layers.Dense(\n",
    "            num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\n",
    "\n",
    "    def call(self, x):\n",
    "        # Ensures we are always using the right batch_size during train/eval.\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        # Conv Feature Extractor.\n",
    "        x = tf.reshape(x, [-1, self.image_size, self.image_size, 3])\n",
    "        x = self.base_model(x)\n",
    "        h = tf.shape(x)[1]\n",
    "        w = tf.shape(x)[2]\n",
    "        c = tf.shape(x)[3]\n",
    "        x = tf.reshape(x, [batch_size, -1, h, w, c])\n",
    "\n",
    "        # 3D Conv to give temporal context to per-frame embeddings.\n",
    "        for bn_layer, conv_layer in zip(self.temporal_bn_layers,\n",
    "                                        self.temporal_conv_layers):\n",
    "            x = conv_layer(x)\n",
    "            x = bn_layer(x)\n",
    "            x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.reduce_max(x, [2, 3])\n",
    "\n",
    "        # Reshape and prepare embs for output.\n",
    "        final_embs = x\n",
    "\n",
    "        # Get self-similarity matrix.\n",
    "        x = get_sims(x, self.temperature)\n",
    "\n",
    "        # 3x3 conv layer on self-similarity matrix.\n",
    "        x = self.conv_3x3_layer(x)\n",
    "        x = tf.reshape(x, [batch_size, self.num_frames, -1])\n",
    "        within_period_x = x\n",
    "\n",
    "        # Period prediction.\n",
    "        x = self.input_projection(x)\n",
    "        x += self.pos_encoding\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x)\n",
    "        x = flatten_sequential_feats(x, batch_size, self.num_frames)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = self.dropout_layer(x)\n",
    "            x = fc_layer(x)\n",
    "\n",
    "        # Within period prediction.\n",
    "        within_period_x = self.input_projection2(within_period_x)\n",
    "        within_period_x += self.pos_encoding2\n",
    "        for transformer_layer in self.transformer_layers2:\n",
    "            within_period_x = transformer_layer(within_period_x)\n",
    "        within_period_x = flatten_sequential_feats(within_period_x,\n",
    "                                                   batch_size,\n",
    "                                                   self.num_frames)\n",
    "        for fc_layer in self.within_period_fc_layers:\n",
    "            within_period_x = self.dropout_layer(within_period_x)\n",
    "            within_period_x = fc_layer(within_period_x)\n",
    "\n",
    "        return x, within_period_x, final_embs\n",
    "\n",
    "    @tf.function\n",
    "    def preprocess(self, imgs):\n",
    "        imgs = tf.cast(imgs, tf.float32)\n",
    "        imgs -= 127.5\n",
    "        imgs /= 127.5\n",
    "        imgs = tf.image.resize(imgs, (self.image_size, self.image_size))\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Repnet\n",
    "\n",
    "def get_repnet_model(logdir):\n",
    "    \"\"\"Returns a trained RepNet model.\n",
    "\n",
    "  Args:\n",
    "    logdir (string): Path to directory where checkpoint will be downloaded.\n",
    "\n",
    "  Returns:\n",
    "    model (Keras model): Trained RepNet model.\n",
    "  \"\"\"\n",
    "    # Check if we are in eager mode.\n",
    "    assert tf.executing_eagerly()\n",
    "\n",
    "    # Models will be called in eval mode.\n",
    "    tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "    # Define RepNet model.\n",
    "    model = ResnetPeriodEstimator()\n",
    "    # tf.function for speed.\n",
    "    model.call = tf.function(model.call, experimental_relax_shapes=True)\n",
    "\n",
    "    # Define checkpoint and checkpoint manager.\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    ckpt_manager = tf.train.CheckpointManager(\n",
    "        ckpt, directory=logdir, max_to_keep=10)\n",
    "    latest_ckpt = ckpt_manager.latest_checkpoint\n",
    "    print('Loading from: ', latest_ckpt)\n",
    "    if not latest_ckpt:\n",
    "        raise ValueError('Path does not have a checkpoint to load.')\n",
    "    # Restore weights.\n",
    "    ckpt.restore(latest_ckpt).expect_partial()\n",
    "\n",
    "    # Pass dummy frames to build graph.\n",
    "    model(tf.random.uniform((1, 64, 112, 112, 3)))\n",
    "    return model\n",
    "\n",
    "\n",
    "def unnorm(query_frame):\n",
    "    min_v = query_frame.min()\n",
    "    max_v = query_frame.max()\n",
    "    query_frame = (query_frame - min_v) / max(1e-7, (max_v - min_v))\n",
    "    return query_frame\n",
    "\n",
    "\n",
    "def create_count_video(frames,\n",
    "                       per_frame_counts,\n",
    "                       within_period,\n",
    "                       score,\n",
    "                       fps,\n",
    "                       output_file,\n",
    "                       delay,\n",
    "                       plot_count=True,\n",
    "                       plot_within_period=False,\n",
    "                       plot_score=False,\n",
    "                       vizualize_reps=False):\n",
    "    \"\"\"Creates video with running count and within period predictions.\n",
    "\n",
    "  Args:\n",
    "    frames (List): List of images in form of NumPy arrays.\n",
    "    per_frame_counts (List): List of floats indicating repetition count for\n",
    "      each frame. This is the rate of repetition for that particular frame.\n",
    "      Summing this list up gives count over entire video.\n",
    "    within_period (List): List of floats indicating score between 0 and 1 if the\n",
    "      frame is inside the periodic/repeating portion of a video or not.\n",
    "    score (float): Score between 0 and 1 indicating the confidence of the\n",
    "      RepNet model's count predictions.\n",
    "    fps (int): Frames per second of the input video. Used to scale the\n",
    "      repetition rate predictions to Hz.\n",
    "    output_file (string): Path of the output video.\n",
    "    delay (integer): Delay between each frame in the output video.\n",
    "    plot_count (boolean): if True plots the count in the output video.\n",
    "    plot_within_period (boolean): if True plots the per-frame within period\n",
    "      scores.\n",
    "    plot_score (boolean): if True plots the confidence of the model along with\n",
    "      count ot within_period scores.\n",
    "  \"\"\"\n",
    "    if output_file[-4:] not in ['.mp4', '.gif']:\n",
    "        raise ValueError('Output format can only be mp4 or gif')\n",
    "\n",
    "    if vizualize_reps:\n",
    "        viz_reps(frames, per_frame_counts, score, output_file, interval=delay, plot_score=plot_score)\n",
    "        return\n",
    "\n",
    "    num_frames = len(frames)\n",
    "\n",
    "    running_counts = np.cumsum(per_frame_counts)\n",
    "    final_count = np.around(running_counts[-1]).astype(np.int)\n",
    "\n",
    "    def count(idx):\n",
    "        return int(np.round(running_counts[idx]))\n",
    "\n",
    "    def rate(idx):\n",
    "        return per_frame_counts[idx] * fps\n",
    "\n",
    "    if plot_count and not plot_within_period:\n",
    "        fig = plt.figure(figsize=(10, 12), tight_layout=True)\n",
    "        im = plt.imshow(unnorm(frames[0]))\n",
    "        if plot_score:\n",
    "            plt.suptitle('Pred Count: %d, '\n",
    "                         'Prob: %0.1f' % (final_count, score),\n",
    "                         fontsize=24)\n",
    "\n",
    "        plt.title('Count 0, Rate: 0', fontsize=24)\n",
    "        plt.axis('off')\n",
    "        plt.grid(b=None)\n",
    "\n",
    "        def update_count_plot(i):\n",
    "            \"\"\"Updates the count plot.\"\"\"\n",
    "            im.set_data(unnorm(frames[i]))\n",
    "            plt.title('Count %d, Rate: %0.4f Hz' % (count(i), rate(i)), fontsize=24)\n",
    "\n",
    "        anim = FuncAnimation(\n",
    "            fig,\n",
    "            update_count_plot,\n",
    "            frames=np.arange(1, num_frames),\n",
    "            interval=delay,\n",
    "            blit=False)\n",
    "        if output_file[-3:] == 'mp4':\n",
    "            anim.save(f\"{output_file[:-4]}_{final_count:02d}.mp4\", dpi=100, fps=None)\n",
    "        elif output_file[-3:] == 'gif':\n",
    "            anim.save(f\"{output_file[:-4]}_{final_count:02d}.gif\", writer='imagemagick', fps=None, dpi=100)\n",
    "\n",
    "    elif plot_within_period:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        im = axs[0].imshow(unnorm(frames[0]))\n",
    "        axs[1].plot(0, within_period[0])\n",
    "        axs[1].set_xlim((0, len(frames)))\n",
    "        axs[1].set_ylim((0, 1))\n",
    "\n",
    "        if plot_score:\n",
    "            plt.suptitle('Pred Count: %d, '\n",
    "                         'Prob: %0.1f' % (final_count, score),\n",
    "                         fontsize=24)\n",
    "\n",
    "        if plot_count:\n",
    "            axs[0].set_title('Count 0, Rate: 0', fontsize=20)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.grid(b=None)\n",
    "\n",
    "        def update_within_period_plot(i):\n",
    "            \"\"\"Updates the within period plot along with count.\"\"\"\n",
    "            im.set_data(unnorm(frames[i]))\n",
    "            axs[0].set_xticks([])\n",
    "            axs[0].set_yticks([])\n",
    "            xs = []\n",
    "            ys = []\n",
    "            if plot_count:\n",
    "                axs[0].set_title('Count %d, Rate: %0.4f Hz' % (count(i), rate(i)),\n",
    "                                 fontsize=20)\n",
    "            for idx in range(i):\n",
    "                xs.append(idx)\n",
    "                ys.append(within_period[int(idx * len(within_period) / num_frames)])\n",
    "            axs[1].clear()\n",
    "            axs[1].set_title('Within Period or Not', fontsize=20)\n",
    "            axs[1].set_xlim((0, num_frames))\n",
    "            axs[1].set_ylim((-0.05, 1.05))\n",
    "            axs[1].plot(xs, ys)\n",
    "\n",
    "        anim = FuncAnimation(\n",
    "            fig,\n",
    "            update_within_period_plot,\n",
    "            frames=np.arange(1, num_frames),\n",
    "            interval=delay,\n",
    "            blit=False,\n",
    "        )\n",
    "        if output_file[-3:] == 'mp4':\n",
    "            anim.save(f\"{output_file[:-4]}_{final_count:02d}.mp4\", dpi=100, fps=None)\n",
    "        elif output_file[-3:] == 'gif':\n",
    "            anim.save(f\"{output_file[:-4]}_{final_count:02d}.gif\", writer='imagemagick', fps=None, dpi=100)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_counts(model, frames, strides, batch_size,\n",
    "               threshold,\n",
    "               within_period_threshold,\n",
    "               constant_speed=False,\n",
    "               median_filter=False,\n",
    "               fully_periodic=False):\n",
    "    \"\"\"Pass frames through model and conver period predictions to count.\"\"\"\n",
    "    seq_len = len(frames)\n",
    "    raw_scores_list = []\n",
    "    scores = []\n",
    "    within_period_scores_list = []\n",
    "\n",
    "    if fully_periodic:\n",
    "        within_period_threshold = 0.0\n",
    "\n",
    "    # normalize each frame by converting pixel values to float, centring around 0 and normalizing it. \n",
    "    frames = model.preprocess(frames)\n",
    "    # print(\"Frames: \", frames)\n",
    "    print(\"\\n\")\n",
    "    # striding to capture variable speed in the video\n",
    "    for stride in strides:\n",
    "        num_batches = int(np.ceil(seq_len / model.num_frames / stride / batch_size))\n",
    "        print(\"stride: {}\".format(stride))\n",
    "        print(\"\\tnum_batches: {}\".format(num_batches))\n",
    "        raw_scores_per_stride = []\n",
    "        within_period_score_stride = []\n",
    "        for batch_idx in range(num_batches):\n",
    "            idxes = tf.range(batch_idx * model.num_frames * stride,\n",
    "                             (batch_idx + batch_size) * model.num_frames * stride,\n",
    "                             stride)\n",
    "            # print(\"\\tbatch_idx : \", batch_idx)\n",
    "            # print(\"\\t\\tidxes : \", idxes)\n",
    "            idxes = tf.clip_by_value(idxes, 0, seq_len - 1)\n",
    "            # print(\"\\t\\tclipped_idxes : \", idxes)\n",
    "            curr_frames = tf.gather(frames, idxes)\n",
    "            # print(\"\\t\\tcurr_frames : \", curr_frames)\n",
    "            curr_frames = tf.reshape(\n",
    "                curr_frames,\n",
    "                [batch_size, model.num_frames, model.image_size, model.image_size, 3])\n",
    "            # print(\"\\t\\tcurr_frames_reshaped : \", curr_frames)\n",
    "            raw_scores, within_period_scores, _ = model(curr_frames)\n",
    "            # print(\"\\t\\traw_scores : \", raw_scores)\n",
    "            # print(\"\\t\\twithin_period_scores : \", within_period_scores)\n",
    "            raw_scores_per_stride.append(np.reshape(raw_scores.numpy(),\n",
    "                                                    [-1, model.num_frames // 2]))\n",
    "            within_period_score_stride.append(np.reshape(within_period_scores.numpy(),\n",
    "                                                         [-1, 1]))\n",
    "        raw_scores_per_stride = np.concatenate(raw_scores_per_stride, axis=0)\n",
    "        raw_scores_list.append(raw_scores_per_stride)\n",
    "        within_period_score_stride = np.concatenate(\n",
    "            within_period_score_stride, axis=0)\n",
    "        pred_score, within_period_score_stride = get_score(\n",
    "            raw_scores_per_stride, within_period_score_stride)\n",
    "        scores.append(pred_score)\n",
    "        within_period_scores_list.append(within_period_score_stride)\n",
    "\n",
    "    # Stride chooser\n",
    "    argmax_strides = np.argmax(scores)\n",
    "    chosen_stride = strides[argmax_strides]\n",
    "    raw_scores = np.repeat(\n",
    "        raw_scores_list[argmax_strides], chosen_stride, axis=0)[:seq_len]\n",
    "    within_period = np.repeat(\n",
    "        within_period_scores_list[argmax_strides], chosen_stride,\n",
    "        axis=0)[:seq_len]\n",
    "    within_period_binary = np.asarray(within_period > within_period_threshold)\n",
    "    if median_filter:\n",
    "        within_period_binary = medfilt(within_period_binary, 5)\n",
    "\n",
    "    # Select Periodic frames\n",
    "    periodic_idxes = np.where(within_period_binary)[0]\n",
    "\n",
    "    if constant_speed:\n",
    "        # Count by averaging predictions. Smoother but\n",
    "        # assumes constant speed.\n",
    "        scores = tf.reduce_mean(\n",
    "            tf.nn.softmax(raw_scores[periodic_idxes], axis=-1), axis=0)\n",
    "        max_period = np.argmax(scores)\n",
    "        pred_score = scores[max_period]\n",
    "        pred_period = chosen_stride * (max_period + 1)\n",
    "        per_frame_counts = (\n",
    "                np.asarray(seq_len * [1. / pred_period]) *\n",
    "                np.asarray(within_period_binary))\n",
    "    else:\n",
    "        # Count each frame. More noisy but adapts to changes in speed.\n",
    "        pred_score = tf.reduce_mean(within_period)\n",
    "        per_frame_periods = tf.argmax(raw_scores, axis=-1) + 1\n",
    "        per_frame_counts = tf.where(\n",
    "            tf.math.less(per_frame_periods, 3),\n",
    "            0.0,\n",
    "            tf.math.divide(1.0,\n",
    "                           tf.cast(chosen_stride * per_frame_periods, tf.float32)),\n",
    "        )\n",
    "        if median_filter:\n",
    "            per_frame_counts = medfilt(per_frame_counts, 5)\n",
    "\n",
    "        per_frame_counts *= np.asarray(within_period_binary)\n",
    "\n",
    "        pred_period = seq_len / np.sum(per_frame_counts)\n",
    "\n",
    "    if pred_score < threshold:\n",
    "        print('No repetitions detected in video as score '\n",
    "              '%0.2f is less than threshold %0.2f.' % (pred_score, threshold))\n",
    "        per_frame_counts = np.asarray(len(per_frame_counts) * [0.])\n",
    "\n",
    "    return (pred_period, pred_score, within_period,\n",
    "            per_frame_counts, chosen_stride)\n",
    "\n",
    "\n",
    "def get_score(period_score, within_period_score):\n",
    "    \"\"\"Combine the period and periodicity scores.\"\"\"\n",
    "    # print(\"\\t\\twithin_period_scores : \", within_period_score)\n",
    "    within_period_score = tf.nn.sigmoid(within_period_score)[:, 0]\n",
    "    # print(\"\\t\\twithin_period_scores_softmax : \", within_period_score)\n",
    "    per_frame_periods = tf.argmax(period_score, axis=-1) + 1\n",
    "    # print(\"\\t\\tperiod_score : \", period_score)\n",
    "    # np.set_printoptions(threshold=sys.maxsize)\n",
    "    # print(\"\\t\\tper_frame_periods : \", per_frame_periods)\n",
    "    # np.set_printoptions(threshold=5)\n",
    "    pred_period_conf = tf.reduce_max(\n",
    "        tf.nn.softmax(period_score, axis=-1), axis=-1)\n",
    "    # print(\"\\t\\tpred_period_conf_reduce_max : \", pred_period_conf)\n",
    "    pred_period_conf = tf.where(\n",
    "        tf.math.less(per_frame_periods, 3), 0.0, pred_period_conf)\n",
    "    # print(\"\\t\\tpred_period_conf_where : \", pred_period_conf)\n",
    "    within_period_score *= pred_period_conf\n",
    "    # np.set_printoptions(threshold=sys.maxsize)\n",
    "    # print(\"\\t\\twithin_period_score : \", within_period_score)\n",
    "    within_period_score = np.sqrt(within_period_score)\n",
    "    pred_score = tf.reduce_mean(within_period_score)\n",
    "    return pred_score, within_period_score\n",
    "\n",
    "\n",
    "def viz_reps(frames,\n",
    "             count,\n",
    "             score,\n",
    "             output_file,\n",
    "             alpha=1.0,\n",
    "             pichart=True,\n",
    "             colormap=plt.cm.PuBu,\n",
    "             num_frames=None,\n",
    "             interval=30,\n",
    "             plot_score=True):\n",
    "\n",
    "    \"\"\"Visualize repetitions.\"\"\"\n",
    "    if isinstance(count, list):\n",
    "        counts = len(frames) * [count / len(frames)]\n",
    "    else:\n",
    "        counts = count\n",
    "    sum_counts = np.cumsum(counts)\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(5, 5),\n",
    "                           tight_layout=True, )\n",
    "\n",
    "    h, w, _ = np.shape(frames[0])\n",
    "    wedge_x = 95 / 112 * w\n",
    "    wedge_y = 17 / 112 * h\n",
    "    wedge_r = 15 / 112 * h\n",
    "    txt_x = 95 / 112 * w\n",
    "    txt_y = 19 / 112 * h\n",
    "    otxt_size = 62 / 112 * h\n",
    "\n",
    "    if plot_score:\n",
    "        plt.title('Score:%.2f' % score, fontsize=20)\n",
    "    im0 = ax.imshow(unnorm(frames[0]))\n",
    "\n",
    "    if not num_frames:\n",
    "        num_frames = len(frames)\n",
    "\n",
    "    if pichart:\n",
    "        wedge1 = matplotlib.patches.Wedge(\n",
    "            center=(wedge_x, wedge_y),\n",
    "            r=wedge_r,\n",
    "            theta1=0,\n",
    "            theta2=0,\n",
    "            color=colormap(1.),\n",
    "            alpha=alpha)\n",
    "        wedge2 = matplotlib.patches.Wedge(\n",
    "            center=(wedge_x, wedge_y),\n",
    "            r=wedge_r,\n",
    "            theta1=0,\n",
    "            theta2=0,\n",
    "            color=colormap(0.5),\n",
    "            alpha=alpha)\n",
    "\n",
    "        ax.add_patch(wedge1)\n",
    "        ax.add_patch(wedge2)\n",
    "        txt = ax.text(\n",
    "            txt_x,\n",
    "            txt_y,\n",
    "            '0',\n",
    "            size=35,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            alpha=0.9,\n",
    "            color='white',\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        txt = ax.text(\n",
    "            txt_x,\n",
    "            txt_y,\n",
    "            '0',\n",
    "            size=otxt_size,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            alpha=0.8,\n",
    "            color=colormap(0.4),\n",
    "        )\n",
    "\n",
    "    def update(i):\n",
    "        \"\"\"Update plot with next frame.\"\"\"\n",
    "        im0.set_data(unnorm(frames[i]))\n",
    "        ctr = int(sum_counts[i])\n",
    "        if pichart:\n",
    "            if ctr % 2 == 0:\n",
    "                wedge1.set_color(colormap(1.0))\n",
    "                wedge2.set_color(colormap(0.5))\n",
    "            else:\n",
    "                wedge1.set_color(colormap(0.5))\n",
    "                wedge2.set_color(colormap(1.0))\n",
    "\n",
    "            wedge1.set_theta1(-90)\n",
    "            wedge1.set_theta2(-90 - 360 * (1 - sum_counts[i] % 1.0))\n",
    "            wedge2.set_theta1(-90 - 360 * (1 - sum_counts[i] % 1.0))\n",
    "            wedge2.set_theta2(-90)\n",
    "\n",
    "        txt.set_text(int(sum_counts[i]))\n",
    "        ax.grid(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.tight_layout()\n",
    "\n",
    "    anim = FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=num_frames,\n",
    "        interval=interval,\n",
    "        blit=False)\n",
    "\n",
    "    final_count = np.around(np.sum(counts)).astype(np.int)\n",
    "\n",
    "    if output_file[-3:] == 'mp4':\n",
    "        anim.save(f\"{output_file[:-4]}_{final_count:02d}.mp4\", dpi=100, fps=None)\n",
    "    elif output_file[-3:] == 'gif':\n",
    "        anim.save(f\"{output_file[:-4]}_{final_count:02d}.gif\", writer='imagemagick', fps=None, dpi=100)\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main\n",
    "\n",
    "def inference(model, args, imgs):\n",
    "    s_time = time.time()\n",
    "    (pred_period, pred_score, within_period,\n",
    "     per_frame_counts, chosen_stride) = get_counts(\n",
    "        model,\n",
    "        imgs,\n",
    "        strides=args[\"STRIDES\"],\n",
    "        batch_size=20,\n",
    "        threshold=args[\"THRESHOLD\"],\n",
    "        within_period_threshold=args[\"WITHIN_PERIOD_THRESHOLD\"],\n",
    "        constant_speed=args[\"CONSTANT_SPEED\"],\n",
    "        median_filter=args[\"MEDIAN_FILTER\"],\n",
    "        fully_periodic=args[\"FULLY_PERIODIC\"])\n",
    "    infer_time = time.time()-s_time\n",
    "    print(\"Inference: \\n\")\n",
    "    print(\"pred_period: \", pred_period)\n",
    "    print(\"pred_score: \", pred_score)\n",
    "#     print(\"within_period: \", within_period)\n",
    "#     print(\"per_frame_counts: \", per_frame_counts)\n",
    "    print(\"chosen_stride: \", chosen_stride)\n",
    "    print(\"infer_time: \", infer_time)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return pred_period, pred_score, within_period, per_frame_counts, chosen_stride, infer_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_file(model, args):\n",
    "    rot_dicts = {\"none\": None,\n",
    "                 \"cw\": cv2.ROTATE_90_CLOCKWISE,\n",
    "                 \"ccw\": cv2.ROTATE_90_COUNTERCLOCKWISE,\n",
    "                 \"180\": cv2.ROTATE_180}\n",
    "\n",
    "    if os.path.isdir(args[\"PATH\"]):\n",
    "        _, _, files = next(os.walk(args[\"PATH\"]))\n",
    "        paths = [f\"{args['PATH']}/{file}\" for file in files]\n",
    "        outs = [f\"{args['OUT']}/{file[:-4]}.mp4\" for file in files]\n",
    "        os.makedirs(args['OUT'], exist_ok=True)\n",
    "    else:\n",
    "        paths = [args['PATH']]\n",
    "        outs = [args['OUT']]\n",
    "\n",
    "    for path, out in tqdm(zip(paths, outs), desc=\"Processing ...\"):\n",
    "        imgs, vid_fps = read_video(path, rot=rot_dicts[args[\"ROT\"]])\n",
    "\n",
    "        (pred_period, pred_score, within_period,\n",
    "         per_frame_counts, chosen_stride, infer_time) = inference(model, args, imgs)\n",
    "        print(f\"Inference time: {infer_time:.02f}s. Visualizing ...\")\n",
    "        create_count_video(imgs, per_frame_counts, within_period, score=pred_score, fps=vid_fps, output_file=out,\n",
    "                           delay=1000/vid_fps, plot_count=True, plot_within_period=True, plot_score=True, vizualize_reps=args['VIZ_REPS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_webcam(model, args, width=500, height=300):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    win_size = int(args[\"WIN_SIZE\"] * fps)\n",
    "    strides = int(win_size * args[\"STRIDE_RATIO\"])\n",
    "\n",
    "    frames = []\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            success, frame_bgr = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb = cv2.resize(frame_rgb, (width, height))\n",
    "\n",
    "            frames.append(frame_rgb)\n",
    "\n",
    "            if len(frames) == win_size:\n",
    "                print(\"win_size:\", win_size)\n",
    "                imgs = np.asarray(frames)\n",
    "                (pred_period, pred_score, within_period,\n",
    "                 per_frame_counts, chosen_stride, infer_time) = inference(model, args, imgs)\n",
    "#                 print(np.cumsum(per_frame_counts))\n",
    "                plt.plot(np.cumsum(per_frame_counts))\n",
    "                plt.show()\n",
    "\n",
    "                while len(frames) > (win_size-strides):\n",
    "                    frames.pop(0)\n",
    "\n",
    "            frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imshow(\"window\", frame_bgr)\n",
    "\n",
    "            key_in = cv2.waitKey(25) & 0xFF\n",
    "\n",
    "            if key_in == ord('q'):\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##@title \n",
    "args = {\n",
    "    # FPS while recording video from webcam.\n",
    "    \"WEBCAM_FPS\" : 16,\n",
    "    # Time in seconds to record video on webcam. \n",
    "    \"RECORDING_TIME_IN_SECONDS\" : 8.0,\n",
    "    # Threshold to consider periodicity in entire video.\n",
    "    \"THRESHOLD\" : 0.2,\n",
    "    # Threshold to consider periodicity for individual frames in video.\n",
    "    \"WITHIN_PERIOD_THRESHOLD\" : 0.2,\n",
    "    \"CONSTANT_SPEED\" : False,\n",
    "    # Use median filtering in time to ignore noisy frames.\n",
    "    \"MEDIAN_FILTER\": True,\n",
    "    \"FULLY_PERIODIC\": False,\n",
    "    \"PLOT_SCORE\" : False,\n",
    "    \"VIZ_FPS\" : 30,\n",
    "    \"WIN_SIZE\" : 3,\n",
    "    # Window stride ratio respect to win-size for webcam mode\n",
    "    \"STRIDE_RATIO\": 0.5,\n",
    "    \"STRIDES\" : [2],\n",
    "    \"VIZ_REPS\" : True,\n",
    "    # Rotate videos. (none, cw, ccw, 180)\n",
    "    \"ROT\": None,\n",
    "    \"PATH\" : \"squat.mp4\",\n",
    "    \"OUT\" : \"./export\",\n",
    "    \"WEIGHTS_FOLDER\" : \"./weights\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from:  ./weights/ckpt-88\n"
     ]
    }
   ],
   "source": [
    "repnet_model = get_model(args[\"WEIGHTS_FOLDER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win_size: 87\n",
      "\n",
      "\n",
      "stride: 2\n",
      "\tnum_batches: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ead572b40d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_webcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepnet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-95822421c6b1>\u001b[0m in \u001b[0;36mprocess_webcam\u001b[0;34m(model, args, width, height)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 (pred_period, pred_score, within_period,\n\u001b[0;32m---> 22\u001b[0;31m                  per_frame_counts, chosen_stride, infer_time) = inference(model, args, imgs)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#                 print(np.cumsum(per_frame_counts))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_frame_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4d91209cdb55>\u001b[0m in \u001b[0;36minference\u001b[0;34m(model, args, imgs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mconstant_speed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CONSTANT_SPEED\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmedian_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MEDIAN_FILTER\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         fully_periodic=args[\"FULLY_PERIODIC\"])\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0minfer_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inference: \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6072f2dff068>\u001b[0m in \u001b[0;36mget_counts\u001b[0;34m(model, frames, strides, batch_size, threshold, within_period_threshold, constant_speed, median_filter, fully_periodic)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 [batch_size, model.num_frames, model.image_size, model.image_size, 3])\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# print(\"\\t\\tcurr_frames_reshaped : \", curr_frames)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mraw_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithin_period_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;31m# print(\"\\t\\traw_scores : \", raw_scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# print(\"\\t\\twithin_period_scores : \", within_period_scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/repcount/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_webcam(repnet_model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
